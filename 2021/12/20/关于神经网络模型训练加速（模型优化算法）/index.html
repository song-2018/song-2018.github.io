<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>关于神经网络模型训练加速（模型优化算法） | Songy's Blog</title><meta name="keywords" content="python,深度学习"><meta name="author" content="Song Yi,2980816415@qq.com"><meta name="copyright" content="Song Yi"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="pyqt那个系统需要进行模型训练的加速方便更好实例化，目前总计了如下几个方法1、Mini-batch gradient descent之前我们介绍的神经网络训练过程是对所有m个样本，称为batch，通过向量化计算方式，同时进行的。如果m很大，例如达到百万数量级，训练速度往往会很慢，因为每次迭代都要对所有样本进行进行求和运算和矩阵运算。我们将这种梯度下降算法称为Batch Gradient Desc">
<meta property="og:type" content="article">
<meta property="og:title" content="关于神经网络模型训练加速（模型优化算法）">
<meta property="og:url" content="http://songy1.cn/2021/12/20/%E5%85%B3%E4%BA%8E%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E5%8A%A0%E9%80%9F%EF%BC%88%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%EF%BC%89/index.html">
<meta property="og:site_name" content="Songy&#39;s Blog">
<meta property="og:description" content="pyqt那个系统需要进行模型训练的加速方便更好实例化，目前总计了如下几个方法1、Mini-batch gradient descent之前我们介绍的神经网络训练过程是对所有m个样本，称为batch，通过向量化计算方式，同时进行的。如果m很大，例如达到百万数量级，训练速度往往会很慢，因为每次迭代都要对所有样本进行进行求和运算和矩阵运算。我们将这种梯度下降算法称为Batch Gradient Desc">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://songy1.cn/img/27.j">
<meta property="article:published_time" content="2021-12-20T11:56:38.000Z">
<meta property="article:modified_time" content="2022-01-08T13:14:30.764Z">
<meta property="article:author" content="Song Yi">
<meta property="article:tag" content="python">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://songy1.cn/img/27.j"><link rel="shortcut icon" href="/img/test.jpg"><link rel="canonical" href="http://songy1.cn/2021/12/20/%E5%85%B3%E4%BA%8E%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E5%8A%A0%E9%80%9F%EF%BC%88%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%EF%BC%89/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '关于神经网络模型训练加速（模型优化算法）',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-01-08 21:14:30'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if (GLOBAL_CONFIG_SITE.isHome && /iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 5.4.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/touxiang.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">13</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">8</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/27.j')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Songy's Blog</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">关于神经网络模型训练加速（模型优化算法）</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-12-20T11:56:38.000Z" title="发表于 2021-12-20 19:56:38">2021-12-20</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-01-08T13:14:30.764Z" title="更新于 2022-01-08 21:14:30">2022-01-08</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">2.9k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>9分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="关于神经网络模型训练加速（模型优化算法）"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="pyqt那个系统需要进行模型训练的加速方便更好实例化，目前总计了如下几个方法"><a href="#pyqt那个系统需要进行模型训练的加速方便更好实例化，目前总计了如下几个方法" class="headerlink" title="pyqt那个系统需要进行模型训练的加速方便更好实例化，目前总计了如下几个方法"></a>pyqt那个系统需要进行模型训练的加速方便更好实例化，目前总计了如下几个方法</h1><h1 id="1、Mini-batch-gradient-descent"><a href="#1、Mini-batch-gradient-descent" class="headerlink" title="1、Mini-batch gradient descent"></a>1、Mini-batch gradient descent</h1><p>之前我们介绍的神经网络训练过程是对所有m个样本，称为batch，通过向量化计算方式，同时进行的。如果m很大，例如达到百万数量级，训练速度往往会很慢，因为每次迭代都要对所有样本进行进行求和运算和矩阵运算。我们将这种梯度下降算法称为Batch Gradient Descent。</p>
<p>为了解决这一问题，我们可以把m个训练样本分成若干个子集，称为mini-batches，这样每个子集包含的数据量就小了，例如只有1000，然后每次在单一子集上进行神经网络训练，速度就会大大提高。这种梯度下降算法叫做Mini-batch Gradient Descent。</p>
<p>假设总的训练样本个数m=5000000，其维度为(nx,m)(nx,m)。将其分成5000个子集，每个mini-batch含有1000个样本。我们将每个mini-batch记为X{t}X{t}，其维度为(nx,1000)(nx,1000)。相应的每个mini-batch的输出记为Y{t}Y{t}，其维度为(1,1000)(1,1000)，且t=1,2,⋯,5000t=1,2,⋯,5000。</p>
<p>这里顺便总结一下我们遇到的神经网络中几类字母的上标含义：</p>
<p>X(i)X(i) ：第i个样本</p>
<p>Z[l]Z[l]：神经网络第ll层网络的线性输出</p>
<p>X{t},Y{t}X{t},Y{t}：第t组mini-batch</p>
<p>Mini-batches Gradient Descent的实现过程是先将总的训练样本分成T个子集（mini-batches），然后对每个mini-batch进行神经网络训练，包括Forward Propagation，Compute Cost Function，Backward Propagation，循环至T个mini-batch都训练完毕。</p>
<p>for  t=1,⋯,T  {for  t=1,⋯,T  {<br>    Forward Propagation    Forward Propagation</p>
<p>ComputeCostFunction    ComputeCostFunction</p>
<p>BackwardPropagation    BackwardPropagation</p>
<p>W:=W−α⋅dW    W:=W−α⋅dW</p>
<p>b:=b−α⋅db    b:=b−α⋅db</p>
<p>}}</p>
<p>经过T次循环之后，所有m个训练样本都进行了梯度下降计算。这个过程，我们称之为经历了一个epoch。对于Batch Gradient Descent而言，一个epoch只进行一次梯度下降算法；而Mini-Batches Gradient Descent，一个epoch会进行T次梯度下降算法。</p>
<p>值得一提的是，对于Mini-Batches Gradient Descent，可以进行多次epoch训练。而且，每次epoch，最好是将总体训练数据重新打乱、重新分成T组mini-batches，这样有利于训练出最佳的神经网络模型</p>
<h1 id="2、分布式训练："><a href="#2、分布式训练：" class="headerlink" title="2、分布式训练："></a>2、分布式训练：</h1><p>哪种并行训练方式更好呢？</p>
<p>答案是，要结合实际情况来看。如果模型过大，当然只能使用模型并行的训练方式。</p>
<p>这里想强调一点的是，如果使用数据并行进行模型的并行训练，那么一定要选用DistributedDataParallel (DDP)而不是DataParallel (DP)。（在Pytorch中有这两种多GPU的数据并行训练模式）</p>
<p>原因是DDP的训练更快，数据传输带来的消耗更少。</p>
<p>DP训练模式下，所有GPU由一个CPU进程控制，模型会被加载到每一块GPU，一旦梯度在GPU0上完成了加权平均，更新后的梯度将被sync到其他所有GPU上。模型的每一层训练都是这样，那么就会产生很多GPU之间的数据传输。并且DP模式只能用在一个node（一个node上最多8块GPU）上。</p>
<p>DDP在每个GPU上（在其自己的进程中）创建模型的孤立副本，并使该GPU仅可使用一部分数据。 然后就像进行N次独立模型训练一样，只是每个模型都会计算梯度，它们都会在模型之间同步梯度……这意味着我们在每批处理中仅一次在GPU之间传输数据。可用于多node的分布式训练。</p>
<h2 id="2-1-数据并行（分布式训练）"><a href="#2-1-数据并行（分布式训练）" class="headerlink" title="2.1 数据并行（分布式训练）"></a>2.1 数据并行（分布式训练）</h2><p>什么是数据并行？<br>以GPU的维度来看，数据并行简单来说就是在并行训练的设备上，对完整训练数据进行分片训练，同一个训练的时间间隔内，不同GPU设备上用各自分片的数据对模型进行训练，其后再进行模型梯度的汇总更新和各GPU间的状态同步。这样做的结果就是在一个训练的时间间隔内，各个GPU设备可以并行地用各自分片的数据进行模型训练，从而大大加速了整体模型的训练。<br>数据并行时，每个GPU设备上保持了同样的模型数据，且一次完整的训练过程包括以下3步：<br>1.CPU负责将不同的训练数据（mini-batch)分别喂给GPU0和GPU1设备；<br>2.不同的显卡设备上存储了完全一致的模型，通过mini-batch数据进行了前向和反向传播；<br>3.位于不同GPU设备上的模型进行权重同步和更新</p>
<h1 id="3、learning-rate-warmeup"><a href="#3、learning-rate-warmeup" class="headerlink" title="3、learning rate warmeup"></a>3、learning rate warmeup</h1><p>如果参数变化非常剧烈，那么式3.0将难以满足，而在训练初期，由于参数随机化，参数离真正的解差距较大，模型的参数变化最为剧烈，为了防止训练初期参数变化过于剧烈，研究人员提出了learning rate warmup。顾名思义，warm up即让模型在进行正式训练前，先使用小的学习率迭代一定epoch，即热身，通过热身降低参数离真正的解的差距，从而防止训练初期参数变化过于剧烈。</p>
<p>warm up有两种策略：</p>
<p>1、Constant warmup：使用固定的学习率α ′ \alpha’α<br>′<br> ，迭代一定次数后，改为正式训练的学习率α \alphaα，在目标检测与语义分割领域有很好的应用（部分pretrain层与随机初始化层一起训练，而非全部随机初始化）<br>2、Gradual warmup：设warm up的迭代次数为m mm，epoch为i ii(0 ≤ i ≤ m 0\leq i\leq m0≤i≤m)，则第i ii个epoch使用的学习率为( i ∗ α ) m \frac{(i*\alpha)}{m}<br>m<br>(i∗α)</p>
<p> ，经过m个epoch后，学习率大小变为正式训练的学习</p>
<h1 id="4、No-bias-decay"><a href="#4、No-bias-decay" class="headerlink" title="4、No bias decay"></a>4、No bias decay</h1><p>weight decay会对网络的所有超参数施加正则化，例如BN层的γ 、 β γ、\betaγ、β、卷积层与全连接层的bias，No bias decay建议只对卷积层与全连接层的权重施加L2正则化，No bias decay降低了weight decay的正则化程度，这在一定程度上会加速网络的收敛。</p>
<h1 id="5、-Zero-γ-γγ"><a href="#5、-Zero-γ-γγ" class="headerlink" title="5、 Zero γ γγ"></a>5、 Zero γ γγ</h1><p>在BN算法中，设经过标准化的特征图为x ^ \hat{x}<br>x<br>^<br> ，则BN层的输出为γ x ^ + β γ\hat{x}+\betaγ<br>x<br>^<br> +β，Zero γ γγ即在初始化时，将γ γγ设置为0。该启发式方案只针对ResNet，此时残差结构的输出等于输入（可以看成是一层），在训练初期，Zero γ γγ相当于降低了网络层数，而浅层网络往往较容易训练。</p>
<h1 id="6、Linear-scaling-learning-rate"><a href="#6、Linear-scaling-learning-rate" class="headerlink" title="6、Linear scaling learning rate"></a>6、Linear scaling learning rate</h1><p>从上一节可知，增大batch，可以加速模型收敛速度，但是大batch会导致模型的泛化性能下降。如果让大batch一次梯度更新后的权重与小batch k kk次梯度更新后的权重一致，那么就能在加速模型收敛的同时，防止模型泛化性能下降，这便是Linear scaling learning rate的出发点。</p>
<p><strong>Linear scaling learning rate</strong>是指当batch size增大n倍时，learning rate也要增大n倍。该理论的推导过程如下：</p>
<h1 id="7、改进梯度下降算法"><a href="#7、改进梯度下降算法" class="headerlink" title="7、改进梯度下降算法"></a>7、改进梯度下降算法</h1><p>RMSprop、Adadelta和Adam被认为是自适应优化算法，因为它们自动更新学习率。使用SGD，你必须手动选择学习速率和动量参数，通常会随着时间的推移而衰减学习速率。</p>
<p> 在实际应用中，自适应优化算法收敛速度快于SGD算法，然而，他们的最终表现通常稍差一些。SGD通常可以达到更好的最小值，从而获得更好的最终精度，但它可能比某些优化器花费的时间要长得多。它还更加依赖于健壮的初始化和学习率衰减策略，这在实践中是非常具有挑战性的。</p>
<p> 因此，如果你需要一些快速的结果，或者只是想测试一种新技术，请使用自适应优化器。我发现Adam很容易使用，因为它对你选择完美的学习速度不是很敏感。如果你想获得绝对最好的最终性能，那么使用SGD + 动量，并使用学习率、衰减和动量值来最大化性能。</p>
<p><strong>两全其美的方法</strong></p>
<p>最近有研究表明，通过从Adam切换到SGD，你可以同时获得两个世界的最佳效果：高速训练，表现一流(<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1712.07628.pdf)%EF%BC%81%E5%85%B6%E6%80%9D%E6%83%B3%E6%98%AF%EF%BC%8C%E8%AE%AD%E7%BB%83%E7%9A%84%E6%97%A9%E6%9C%9F%E9%98%B6%E6%AE%B5%E5%AE%9E%E9%99%85%E4%B8%8A%E6%98%AFSGD%E5%AF%B9%E5%8F%82%E6%95%B0%E8%B0%83%E4%BC%98%E5%92%8C%E5%88%9D%E5%A7%8B%E5%8C%96%E9%9D%9E%E5%B8%B8%E6%95%8F%E6%84%9F%E7%9A%84%E6%97%B6%E5%80%99%E3%80%82%E5%9B%A0%E6%AD%A4%EF%BC%8C%E6%88%91%E4%BB%AC%E5%8F%AF%E4%BB%A5%E4%BB%8E%E4%BD%BF%E7%94%A8Adam%E5%BC%80%E5%A7%8B%E6%88%91%E4%BB%AC%E7%9A%84%E5%9F%B9%E8%AE%AD%EF%BC%8C%E8%BF%99%E5%B0%86%E4%BD%BF%E4%BD%A0%E5%9C%A8%E4%B8%8D%E5%BF%85%E6%8B%85%E5%BF%83%E5%88%9D%E5%A7%8B%E5%8C%96%E5%92%8C%E5%8F%82%E6%95%B0%E4%BC%98%E5%8C%96%E7%9A%84%E6%83%85%E5%86%B5%E4%B8%8B%E8%B5%B0%E5%BE%97%E5%BE%88%E8%BF%9C%E3%80%82%E7%84%B6%E5%90%8E%EF%BC%8C%E4%B8%80%E6%97%A6Adam%E8%AE%A9%E6%88%91%E4%BB%AC%E5%BC%80%E5%A7%8B%EF%BC%8C%E6%88%91%E4%BB%AC%E5%8F%AF%E4%BB%A5%E5%88%87%E6%8D%A2%E5%88%B0SGD">https://arxiv.org/pdf/1712.07628.pdf)！其思想是，训练的早期阶段实际上是SGD对参数调优和初始化非常敏感的时候。因此，我们可以从使用Adam开始我们的培训，这将使你在不必担心初始化和参数优化的情况下走得很远。然后，一旦Adam让我们开始，我们可以切换到SGD</a> +动量优化，以达到最佳性能！</p>
<p><img src="https://s2.loli.net/2021/12/20/4CAloe3zRvuXL9y.png" alt="image-20211028071545280.png"></p>
<p>Adam vs SGD的表现。Adam由于具有鲁棒性和自适应学习率，在开始时表现较好，而SGD最终达到了较好的全局最小值。</p>
<h1 id="9、知识蒸馏"><a href="#9、知识蒸馏" class="headerlink" title="9、知识蒸馏"></a>9、知识蒸馏</h1><p>我们最后提出的改善模型推理时间的选项是通过知识蒸馏。假设我们有一个大模型（或模型的集合），它的预测精度很高，但其推理速度并不理想。Knowledge Distillation 建议通过使用我们的大模型作为训练器来训练一个具有较少参数的较小模型。这实质上是训练我们的小模型输出与我们的大模型或集成相同的预测。这样做的一个很大优势是我们不仅限于使用标记数据。</p>
<p>请注意，虽然我们的准确性可能会受到影响，但我们应该能够从中获得不错的速度提升。不幸的是，我们没有愉快地自己实施这种方法。但是知识蒸馏最近非常流行，并已用于对象分类、对象检测、声学模型和 NLP等。如果你想了解更多关于 知识蒸馏的信息，请查看Geoffrey Hinton等人的这篇论文。</p>
<h1 id="10、-Learning-rate-decay"><a href="#10、-Learning-rate-decay" class="headerlink" title="10、 Learning rate decay"></a>10、 Learning rate decay</h1><p>减小学习因子αα也能有效提高神经网络训练速度，这种方法被称为learning rate decay。</p>
<p>Learning rate decay就是随着迭代次数增加，学习因子αα逐渐减小。下面用图示的方式来解释这样做的好处。下图中，蓝色折线表示使用恒定的学习因子αα，由于每次训练αα相同，步进长度不变，在接近最优值处的振荡也大，在最优值附近较大范围内振荡，与最优值距离就比较远。绿色折线表示使用不断减小的αα，随着训练次数增加，αα逐渐减小，步进长度减小，使得能够在最优值处较小范围内微弱振荡，不断逼近最优值。相比较恒定的αα来说，learning rate decay更接近最优值。</p>
<p><img src="https://s2.loli.net/2021/12/20/LBusdpiatSIxNwK.png" alt="image-20211030105823471.png"></p>
<h1 id="11、权值共享"><a href="#11、权值共享" class="headerlink" title="11、权值共享"></a>11、权值共享</h1><h1 id="12、混合精度计算："><a href="#12、混合精度计算：" class="headerlink" title="12、混合精度计算："></a>12、混合精度计算：</h1><h1 id="13、动量梯度下降"><a href="#13、动量梯度下降" class="headerlink" title="13、动量梯度下降"></a>13、动量梯度下降</h1></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:2980816415@qq.com">Song Yi</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://songy1.cn/2021/12/20/%E5%85%B3%E4%BA%8E%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E5%8A%A0%E9%80%9F%EF%BC%88%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%EF%BC%89/">http://songy1.cn/2021/12/20/关于神经网络模型训练加速（模型优化算法）/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://songy1.cn" target="_blank">Songy's Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/python/">python</a><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></div><div class="post_share"><div class="social-share" data-image="/img/27.j" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2021/12/25/%E4%B8%80%E4%BA%9B%E5%9B%B0%E6%83%91/"><img class="prev-cover" src="/img/19.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">一些困惑</div></div></a></div><div class="next-post pull-right"><a href="/2021/12/20/flask/"><img class="next-cover" src="/img/26.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">flask</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2021/12/19/%E5%85%B3%E4%BA%8Egpu%E6%BA%A2%E5%87%BA/" title="关于gpu溢出"><img class="cover" src="/img/19.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-12-19</div><div class="title">关于gpu溢出</div></div></a></div><div><a href="/2021/12/19/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BB%A5%E5%8F%8A%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B/" title="卷积神经网络以及训练过程"><img class="cover" src="/img/19.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-12-19</div><div class="title">卷积神经网络以及训练过程</div></div></a></div><div><a href="/2021/12/19/%E6%B7%B1%E5%BA%A6%E7%BD%91%E7%BB%9C%E5%BB%BA%E6%A8%A1%E7%B3%BB%E7%BB%9F-ahau/" title="深度网络建模系统-ahau"><img class="cover" src="/img/19.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-12-19</div><div class="title">深度网络建模系统-ahau</div></div></a></div><div><a href="/2021/12/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E4%BA%9B%E7%9F%A5%E8%AF%86%E7%82%B9/" title="机器学习的一些知识点"><img class="cover" src="/img/19.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-12-19</div><div class="title">机器学习的一些知识点</div></div></a></div><div><a href="/2021/12/20/%E6%B7%B7%E5%90%88%E7%B2%BE%E5%BA%A6%E5%8A%A0%E9%80%9F/" title="混合精度加速"><img class="cover" src="/img/15.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-12-20</div><div class="title">混合精度加速</div></div></a></div><div><a href="/2021/12/20/flask/" title="flask"><img class="cover" src="/img/26.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-12-20</div><div class="title">flask</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/touxiang.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Song Yi</div><div class="author-info__description">这个世界上平凡人改变命运的捷径，一个是教育，一个是联姻</div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">13</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">8</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/song-2018" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="qq:2980816415@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#pyqt%E9%82%A3%E4%B8%AA%E7%B3%BB%E7%BB%9F%E9%9C%80%E8%A6%81%E8%BF%9B%E8%A1%8C%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E7%9A%84%E5%8A%A0%E9%80%9F%E6%96%B9%E4%BE%BF%E6%9B%B4%E5%A5%BD%E5%AE%9E%E4%BE%8B%E5%8C%96%EF%BC%8C%E7%9B%AE%E5%89%8D%E6%80%BB%E8%AE%A1%E4%BA%86%E5%A6%82%E4%B8%8B%E5%87%A0%E4%B8%AA%E6%96%B9%E6%B3%95"><span class="toc-number">1.</span> <span class="toc-text">pyqt那个系统需要进行模型训练的加速方便更好实例化，目前总计了如下几个方法</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#1%E3%80%81Mini-batch-gradient-descent"><span class="toc-number">2.</span> <span class="toc-text">1、Mini-batch gradient descent</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2%E3%80%81%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83%EF%BC%9A"><span class="toc-number">3.</span> <span class="toc-text">2、分布式训练：</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1-%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C%EF%BC%88%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83%EF%BC%89"><span class="toc-number">3.1.</span> <span class="toc-text">2.1 数据并行（分布式训练）</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3%E3%80%81learning-rate-warmeup"><span class="toc-number">4.</span> <span class="toc-text">3、learning rate warmeup</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#4%E3%80%81No-bias-decay"><span class="toc-number">5.</span> <span class="toc-text">4、No bias decay</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#5%E3%80%81-Zero-%CE%B3-%CE%B3%CE%B3"><span class="toc-number">6.</span> <span class="toc-text">5、 Zero γ γγ</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#6%E3%80%81Linear-scaling-learning-rate"><span class="toc-number">7.</span> <span class="toc-text">6、Linear scaling learning rate</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#7%E3%80%81%E6%94%B9%E8%BF%9B%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95"><span class="toc-number">8.</span> <span class="toc-text">7、改进梯度下降算法</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#9%E3%80%81%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F"><span class="toc-number">9.</span> <span class="toc-text">9、知识蒸馏</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#10%E3%80%81-Learning-rate-decay"><span class="toc-number">10.</span> <span class="toc-text">10、 Learning rate decay</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#11%E3%80%81%E6%9D%83%E5%80%BC%E5%85%B1%E4%BA%AB"><span class="toc-number">11.</span> <span class="toc-text">11、权值共享</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#12%E3%80%81%E6%B7%B7%E5%90%88%E7%B2%BE%E5%BA%A6%E8%AE%A1%E7%AE%97%EF%BC%9A"><span class="toc-number">12.</span> <span class="toc-text">12、混合精度计算：</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#13%E3%80%81%E5%8A%A8%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">13.</span> <span class="toc-text">13、动量梯度下降</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2022/01/05/2021/" title="2021"><img src="/img/28.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="2021"/></a><div class="content"><a class="title" href="/2022/01/05/2021/" title="2021">2021</a><time datetime="2022-01-05T06:48:34.000Z" title="发表于 2022-01-05 14:48:34">2022-01-05</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/12/25/%E4%B8%80%E4%BA%9B%E5%9B%B0%E6%83%91/" title="一些困惑"><img src="/img/19.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="一些困惑"/></a><div class="content"><a class="title" href="/2021/12/25/%E4%B8%80%E4%BA%9B%E5%9B%B0%E6%83%91/" title="一些困惑">一些困惑</a><time datetime="2021-12-24T16:35:28.000Z" title="发表于 2021-12-25 00:35:28">2021-12-25</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/12/20/%E5%85%B3%E4%BA%8E%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E5%8A%A0%E9%80%9F%EF%BC%88%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%EF%BC%89/" title="关于神经网络模型训练加速（模型优化算法）"><img src="/img/27.j" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="关于神经网络模型训练加速（模型优化算法）"/></a><div class="content"><a class="title" href="/2021/12/20/%E5%85%B3%E4%BA%8E%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E5%8A%A0%E9%80%9F%EF%BC%88%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%EF%BC%89/" title="关于神经网络模型训练加速（模型优化算法）">关于神经网络模型训练加速（模型优化算法）</a><time datetime="2021-12-20T11:56:38.000Z" title="发表于 2021-12-20 19:56:38">2021-12-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/12/20/flask/" title="flask"><img src="/img/26.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="flask"/></a><div class="content"><a class="title" href="/2021/12/20/flask/" title="flask">flask</a><time datetime="2021-12-20T11:46:56.000Z" title="发表于 2021-12-20 19:46:56">2021-12-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/12/20/about%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" title="about深度学习"><img src="/img/19.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="about深度学习"/></a><div class="content"><a class="title" href="/2021/12/20/about%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" title="about深度学习">about深度学习</a><time datetime="2021-12-20T11:41:04.000Z" title="发表于 2021-12-20 19:41:04">2021-12-20</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('/img/27.j')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2022 By Song Yi</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a target="_blank" rel="noopener" href="https://butterfly.js.org/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())</script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>